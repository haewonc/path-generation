{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join as pjoin\n",
    "import geopandas as gpd\n",
    "from cartoframes.viz import *\n",
    "from shapely.geometry import MultiPoint, Point\n",
    "import datetime\n",
    "import osmnx as ox\n",
    "from core.urban_osm import UrbanOSM\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set postfix for the saved dataset filename\n",
    "POSTFIX = 'ua' \n",
    "DATASET_ROOT = '../dataset/'\n",
    "DATASET = 'metr-la'\n",
    "RESULT_ROOT = pjoin('../results/', DATASET)\n",
    "OSM_FILE_PATH = pjoin(RESULT_ROOT, 'osm_graph', 'drive.graphml')\n",
    "\n",
    "assert DATASET in ['metr-la', 'pems-bay', 'pemsd7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(RESULT_ROOT):\n",
    "    os.makedirs(RESULT_ROOT)\n",
    "if not os.path.exists(pjoin(RESULT_ROOT, 'osm_graph')):\n",
    "    os.makedirs(pjoin(RESULT_ROOT, 'osm_graph'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df, sensor_ids, sensor_df, sensor_id_to_ind, adj_mx = load_dataset(DATASET_ROOT, DATASET)\n",
    "num_sensors = len(sensor_ids)\n",
    "ind_to_sensor_id = {v:k for k, v in sensor_id_to_ind.items()}\n",
    "\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Save original Adjacency Matrix: ', np.count_nonzero(adj_mx))\n",
    "org_fname = pjoin(RESULT_ROOT, 'original_adj_mx.pkl')\n",
    "if not os.path.isfile(org_fname):\n",
    "    with open(org_fname, 'wb') as f:\n",
    "        pickle.dump([sensor_ids, sensor_id_to_ind, adj_mx], f, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize OSM with datset\n",
    "This might take few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urbanosm = UrbanOSM(sensor_df, OSM_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urbanosm.match_sensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urbanosm.setup_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urbanosm.navigate_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urbanosm.setup_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basemap = {\n",
    "    'style': 'mapbox://styles/mapbox/streets-v9',\n",
    "    'token': 'pk.eyJ1IjoiaHNtNjkxMSIsImEiOiJjazl0and6aDUwOWF2M2RvemdrYjllczV3In0.qGmaAF6v-1LAF9C-dnMLBg'\n",
    "}\n",
    "urbanosm.navigate_example(basemap=basemap, show_grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urbanosm.generate_paths(pjoin(RESULT_ROOT, f'generated_paths_{POSTFIX}.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urbanosm.setup_sensor_words()\n",
    "urbanosm.setup_path_sentences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urbanosm.save_geojson(pjoin(RESULT_ROOT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjacency Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node2Vec Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2v_fname = pjoin(RESULT_ROOT, f'n2v_sim_{POSTFIX}.pkl')\n",
    "if os.path.isfile(n2v_fname):\n",
    "    with open(n2v_fname, 'rb') as f:\n",
    "        _, _, sim_array = pickle.load(f)\n",
    "    print('Loaded from '+n2v_fname)\n",
    "else:\n",
    "    vector_size = 64\n",
    "    sentences = [sent.split() for sent in urbanosm.path_sentences]\n",
    "    model = Word2Vec(sentences, window=7, min_count=1, workers=4, vector_size=vector_size)\n",
    "\n",
    "    import numpy as np\n",
    "    wv_array = []\n",
    "    for sid in data_df.columns:\n",
    "        q = f'S{sid}'\n",
    "        if q in model.wv:\n",
    "            wv_array.append(model.wv[q])\n",
    "        else:\n",
    "            wv_array.append(np.zeros(vector_size))\n",
    "            \n",
    "    wv_array = np.array(wv_array)\n",
    "\n",
    "    def cosine_similarity(vector1, vector2):\n",
    "        dot_product = np.dot(vector1, vector2)\n",
    "        if dot_product == 0:\n",
    "            return -1\n",
    "        magnitude1 = np.linalg.norm(vector1)\n",
    "        magnitude2 = np.linalg.norm(vector2)\n",
    "        cosine_similarity = dot_product / (magnitude1 * magnitude2)\n",
    "        return cosine_similarity\n",
    "\n",
    "    sim_array = np.eye(num_sensors)\n",
    "    for i in range(wv_array.shape[0]):\n",
    "        for j in range(i+1, wv_array.shape[0]):\n",
    "            sim_array[j, i] = sim_array[i, j] = cosine_similarity(wv_array[i], wv_array[j])\n",
    "\n",
    "    with open(n2v_fname, 'wb') as f:\n",
    "        pickle.dump([sensor_ids, sensor_id_to_ind, sim_array], f, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-orccurence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccur_fname = pjoin(RESULT_ROOT, f'cooccur_sim_{POSTFIX}.pkl')\n",
    "if os.path.isfile(cooccur_fname):\n",
    "    with open(cooccur_fname, 'rb') as file:\n",
    "        _, _, cooccur_matrix = pickle.load(file)\n",
    "    print('Loaded from '+cooccur_fname)\n",
    "else:\n",
    "    sentences = urbanosm.path_sentences\n",
    "\n",
    "    co_occurrence_vectors = pd.DataFrame(\n",
    "        np.zeros([len(sensor_ids), len(sensor_ids)]),\n",
    "        index = ['S'+s for s in sensor_ids],\n",
    "        columns = ['S'+s for s in sensor_ids]\n",
    "    )\n",
    "\n",
    "    word_count = dict()\n",
    "    word_co_occur = dict()\n",
    "    for sent in tqdm.tqdm(sentences):\n",
    "        ext_sent = [w for w in sent.split() if w[0] == 'S']\n",
    "        for i, w in enumerate(ext_sent):\n",
    "            word_count.setdefault(w, 0)\n",
    "            co_occurrence_vectors.loc[w, w] +=1\n",
    "            \n",
    "        for w2 in ext_sent[i+1:]:\n",
    "                if w != w2:\n",
    "                    co_occurrence_vectors.loc[w, w2] += 1\n",
    "                    co_occurrence_vectors.loc[w2, w] += 1\n",
    "\n",
    "    cooccur_matrix = np.eye(num_sensors)\n",
    "\n",
    "    for i in range(num_sensors):\n",
    "        for j in range(i, num_sensors):\n",
    "            w = 'S'+ind_to_sensor_id[i]\n",
    "            w2 = 'S'+ind_to_sensor_id[j]\n",
    "            wc1 = co_occurrence_vectors.loc[w, w]\n",
    "            wc2 = co_occurrence_vectors.loc[w2, w2]\n",
    "            cooccur_matrix[j, i] = cooccur_matrix[i, j] = co_occurrence_vectors.loc[w2, w]/((wc1*wc2)**.5+1)\n",
    "    \n",
    "    with open(f'{DATASET}/cooccur_sim_{POSTFIX}.pkl', 'wb') as f:\n",
    "        pickle.dump([sensor_ids, sensor_id_to_ind, cooccur_matrix], f, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reachable Distance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_fname = pjoin(RESULT_ROOT, f'dist_meters_{POSTFIX}.pkl')\n",
    "if os.path.isfile(dist_fname):\n",
    "    with open(dist_fname, 'rb') as file:\n",
    "        _, _, dist_mat = pickle.load(file)\n",
    "    print('Loaded from '+dist_fname)\n",
    "else:\n",
    "    sid_dist_dict = dict()\n",
    "\n",
    "    for path_sentence in tqdm.tqdm(urbanosm.path_sentences):\n",
    "        co_sensors = [node for node in path_sentence.split() if node[0] == 'S']\n",
    "        for i, sid1 in enumerate(co_sensors[:-1]):\n",
    "            sid2 = co_sensors[i+1]\n",
    "            if sid1 in sid_dist_dict and sid2 in sid_dist_dict[sid1]:\n",
    "                continue\n",
    "            track_paths = urbanosm.track_path(path_sentence, sid1, sid2)\n",
    "            between_sid_dist = urbanosm.sid_dist(sid1, sid2, track_paths)\n",
    "            sid_dist_dict.setdefault(sid1, dict())\n",
    "            sid_dist_dict[sid1][sid2] = between_sid_dist\n",
    "        \n",
    "        for i, sid in enumerate(co_sensors[:-1]):\n",
    "            cum_dist = 0\n",
    "            psid = sid\n",
    "            for qsid in co_sensors[i+1:]:\n",
    "                cum_dist += sid_dist_dict[psid][qsid]\n",
    "                psid = qsid\n",
    "                if sid in sid_dist_dict and qsid in sid_dist_dict[sid]:\n",
    "                    continue\n",
    "                else:\n",
    "                    sid_dist_dict[sid][qsid] = cum_dist\n",
    "    \n",
    "    dist_mat = np.zeros((len(sensor_ids), len(sensor_ids)))\n",
    "    dist_mat.fill(np.inf)\n",
    "    np.fill_diagonal(dist_mat, 0)\n",
    "    for k1 in sid_dist_dict:\n",
    "        for k2 in sid_dist_dict[k1]:        \n",
    "            ii = sensor_id_to_ind[k1[1:]]\n",
    "            jj = sensor_id_to_ind[k2[1:]]\n",
    "            val = sid_dist_dict[k1][k2]\n",
    "            dist_mat[jj, ii] = val\n",
    "    \n",
    "    with open(dist_fname, 'wb') as f:\n",
    "        pickle.dump([sensor_ids, sensor_id_to_ind, dist_mat], f, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mat[dist_mat > MILE_TO_METER*80] = np.inf\n",
    "dist_vals_meters = dist_mat[~np.isinf(dist_mat)].flatten()\n",
    "dist_sigma = 5*MILE_TO_METER\n",
    "dist_normed = np.exp(-np.square(dist_mat / dist_sigma))\n",
    "final_adj_mx = dist_normed*cooccur_matrix\n",
    "\n",
    "normed_fname = pjoin(RESULT_ROOT, f'new_dist_sim_{POSTFIX}.pkl')\n",
    "final_fname = pjoin(RESULT_ROOT, f'urban_activity_sim_{POSTFIX}.pkl')\n",
    "\n",
    "if not os.path.isfile(normed_fname):\n",
    "    with open(normed_fname, 'wb') as f:\n",
    "        pickle.dump([sensor_ids, sensor_id_to_ind, dist_normed], f, protocol=2)\n",
    "\n",
    "if not os.path.isfile(final_fname):   \n",
    "    with open(final_fname, 'wb') as f:\n",
    "        pickle.dump([sensor_ids, sensor_id_to_ind, final_adj_mx], f, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = [adj_mx, sim_array, cooccur_matrix, dist_mat, dist_normed, final_adj_mx]\n",
    "titles = ['Original', 'Node2Vec', 'Co-occurrence', \n",
    "          'Distance', 'Normalized', 'Final']\n",
    "for m, t in zip(matrices, titles):\n",
    "    print(f'{t}\\t distance_graph_loaded', np.count_nonzero(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))\n",
    "\n",
    "for ax, m, t in zip(axes.flatten(), matrices, titles):\n",
    "    ax.matshow(m)\n",
    "    ax.set_title(t)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "path-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
